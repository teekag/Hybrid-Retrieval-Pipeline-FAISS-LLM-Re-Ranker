{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ LLM Integration for Answer Generation\n",
    "\n",
    "This notebook demonstrates how to integrate a Large Language Model (LLM) with our retrieval pipeline to generate answers based on retrieved documents. We'll explore:\n",
    "\n",
    "1. **Prompt Engineering** - Crafting effective prompts for answer generation\n",
    "2. **Retrieval-Augmented Generation** - Using retrieved documents to ground LLM responses\n",
    "3. **Comparison Analysis** - Evaluating answers with and without retrieval\n",
    "\n",
    "This completes the RAG (Retrieval-Augmented Generation) pipeline by adding the generation component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the src directory to the path\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.rag_pipeline import RAGPipeline\n",
    "from src.llm_generator import LLMGenerator\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Set OpenAI API key - replace with your own or use environment variable\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Check if API key is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY environment variable not set.\")\n",
    "    print(\"Please set your API key using os.environ['OPENAI_API_KEY'] = 'your-key-here'\")\n",
    "    print(\"or export it in your environment before running this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize RAG Pipeline and LLM Generator\n",
    "\n",
    "First, we'll set up our retrieval pipeline and LLM generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Initialize RAG Pipeline\n",
    "rag_pipeline = RAGPipeline()\n",
    "\n",
    "# Load documents\n",
    "rag_pipeline.load_documents(str(PROCESSED_DIR / \"processed_chunks.json\"))\n",
    "\n",
    "# Initialize LLM Generator\n",
    "llm_generator = LLMGenerator(\n",
    "    model=\"gpt-3.5-turbo\",  # You can change to \"gpt-4\" for better quality\n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(f\"RAG Pipeline initialized with {len(rag_pipeline.documents)} documents\")\n",
    "print(f\"LLM Generator initialized with model: {llm_generator.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Prompt Templates\n",
    "\n",
    "Let's define some custom prompt templates for different types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Define custom prompt templates\n",
    "standard_prompt = \"\"\"\n",
    "Answer the following question based on the provided context information. \n",
    "If the answer cannot be determined from the context, say so clearly.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "detailed_prompt = \"\"\"\n",
    "You are an expert in sports science and athletic performance. \n",
    "Provide a detailed and scientifically accurate answer to the following question \n",
    "based ONLY on the provided context information.\n",
    "\n",
    "If the answer cannot be fully determined from the context, clearly state what is known \n",
    "from the context and what additional information would be needed.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "DETAILED ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "summarization_prompt = \"\"\"\n",
    "Summarize the key information from the provided context that is relevant to the question.\n",
    "Be concise but comprehensive, focusing on the most important points.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "# Map of prompt types\n",
    "prompt_templates = {\n",
    "    \"standard\": standard_prompt,\n",
    "    \"detailed\": detailed_prompt,\n",
    "    \"summary\": summarization_prompt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full RAG Pipeline: Retrieval + Generation\n",
    "\n",
    "Now let's combine retrieval and generation to answer queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def full_rag_pipeline(query: str, top_k: int = 5, prompt_type: str = \"standard\"):\n",
    "    \"\"\"Run the full RAG pipeline: retrieval + generation.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieval_results = rag_pipeline.query(query, top_k=top_k, rerank=True)\n",
    "    \n",
    "    # Step 2: Format documents for the LLM\n",
    "    retrieved_docs = []\n",
    "    for doc in retrieval_results['results']:\n",
    "        retrieved_docs.append({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"content\": doc[\"content\"],\n",
    "            \"metadata\": {\n",
    "                \"source\": doc.get(\"metadata\", {}).get(\"source\", \"Unknown\"),\n",
    "                \"score\": doc[\"score\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Step 3: Generate answer using LLM\n",
    "    prompt_template = prompt_templates.get(prompt_type, prompt_templates[\"standard\"])\n",
    "    answer_result = llm_generator.generate_answer(query, retrieved_docs, prompt_template)\n",
    "    \n",
    "    # Calculate total time\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Combine results\n",
    "    full_result = {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer_result[\"answer\"],\n",
    "        \"retrieval_results\": retrieval_results,\n",
    "        \"prompt_type\": prompt_type,\n",
    "        \"model\": llm_generator.model,\n",
    "        \"timing\": {\n",
    "            \"retrieval_time\": retrieval_results[\"timing\"][\"total_time\"],\n",
    "            \"generation_time\": total_time - retrieval_results[\"timing\"][\"total_time\"],\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return full_result\n",
    "\n",
    "# Test the full pipeline with a sample query\n",
    "sample_query = \"How does sleep quality affect athletic performance and recovery?\"\n",
    "result = full_rag_pipeline(sample_query, top_k=3, prompt_type=\"detailed\")\n",
    "\n",
    "# Display the answer\n",
    "print(f\"Query: {result['query']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Sources:\")\n",
    "for i, doc in enumerate(result['retrieval_results']['results']):\n",
    "    print(f\"  {i+1}. {doc['id']} (Score: {doc['score']:.3f})\")\n",
    "print(f\"\\nTiming: Retrieval {result['timing']['retrieval_time']:.3f}s, Generation {result['timing']['generation_time']:.3f}s, Total {result['timing']['total_time']:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Answers With and Without Retrieval\n",
    "\n",
    "Let's compare answers generated with retrieved context vs. without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def compare_with_without_retrieval(query: str, top_k: int = 3):\n",
    "    \"\"\"Compare answers generated with and without retrieval context.\"\"\"\n",
    "    # Get retrieval results\n",
    "    retrieval_results = rag_pipeline.query(query, top_k=top_k, rerank=True)\n",
    "    \n",
    "    # Format documents for the LLM\n",
    "    retrieved_docs = []\n",
    "    for doc in retrieval_results['results']:\n",
    "        retrieved_docs.append({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"content\": doc[\"content\"],\n",
    "            \"metadata\": {\n",
    "                \"source\": doc.get(\"metadata\", {}).get(\"source\", \"Unknown\"),\n",
    "                \"score\": doc[\"score\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Compare answers\n",
    "    comparison = llm_generator.compare_with_without_retrieval(query, retrieved_docs)\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Test with a few queries\n",
    "comparison_queries = [\n",
    "    \"What is HRV and how does it relate to recovery?\",\n",
    "    \"How can athletes optimize their sleep for better performance?\",\n",
    "    \"What are the signs of overtraining syndrome?\"\n",
    "]\n",
    "\n",
    "for query in comparison_queries:\n",
    "    print(f\"\\n{'='*80}\\nQUERY: {query}\\n{'='*80}\\n\")\n",
    "    \n",
    "    comparison = compare_with_without_retrieval(query)\n",
    "    \n",
    "    print(\"WITH RETRIEVAL:\\n\")\n",
    "    print(comparison['with_retrieval']['answer'])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    print(\"WITHOUT RETRIEVAL:\\n\")\n",
    "    print(comparison['without_retrieval']['answer'])\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Answer Quality\n",
    "\n",
    "Let's analyze the quality of answers generated with different prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Define a set of test queries\n",
    "test_queries = [\n",
    "    \"What factors affect HRV measurements?\",\n",
    "    \"How can I use HRV to guide my training?\",\n",
    "    \"What is the relationship between sleep and recovery?\",\n",
    "    \"How should nutrition be adjusted for high-intensity training?\",\n",
    "    \"What are the best strategies for monitoring training load?\"\n",
    "]\n",
    "\n",
    "# Test different prompt templates\n",
    "results = []\n",
    "\n",
    "for query in test_queries:\n",
    "    for prompt_type in prompt_templates.keys():\n",
    "        print(f\"Processing query: '{query}' with prompt type: {prompt_type}\")\n",
    "        result = full_rag_pipeline(query, top_k=3, prompt_type=prompt_type)\n",
    "        \n",
    "        # Store result summary\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            \"answer_length\": len(result[\"answer\"]),\n",
    "            \"retrieval_time\": result[\"timing\"][\"retrieval_time\"],\n",
    "            \"generation_time\": result[\"timing\"][\"generation_time\"],\n",
    "            \"total_time\": result[\"timing\"][\"total_time\"]\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\nSummary Statistics:\")\n",
    "summary = results_df.groupby('prompt_type').agg({\n",
    "    'answer_length': ['mean', 'std'],\n",
    "    'retrieval_time': 'mean',\n",
    "    'generation_time': 'mean',\n",
    "    'total_time': 'mean'\n",
    "})\n",
    "display(summary)\n",
    "\n",
    "# Visualize answer length by prompt type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='prompt_type', y='answer_length', data=results_df)\n",
    "plt.title('Answer Length by Prompt Type')\n",
    "plt.xlabel('Prompt Type')\n",
    "plt.ylabel('Answer Length (characters)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(OUTPUT_DIR / \"answer_length_by_prompt.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Visualize timing by prompt type\n",
    "timing_data = results_df.melt(\n",
    "    id_vars=['prompt_type', 'query'],\n",
    "    value_vars=['retrieval_time', 'generation_time'],\n",
    "    var_name='timing_type',\n",
    "    value_name='time_seconds'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='prompt_type', y='time_seconds', hue='timing_type', data=timing_data)\n",
    "plt.title('Processing Time by Prompt Type')\n",
    "plt.xlabel('Prompt Type')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(OUTPUT_DIR / \"processing_time_by_prompt.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving Results for Future Analysis\n",
    "\n",
    "Let's save our results for future reference and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Create a comprehensive example with all prompt types\n",
    "comprehensive_query = \"How can athletes use HRV to optimize their training and recovery?\"\n",
    "comprehensive_results = {}\n",
    "\n",
    "for prompt_type in prompt_templates.keys():\n",
    "    result = full_rag_pipeline(comprehensive_query, top_k=5, prompt_type=prompt_type)\n",
    "    comprehensive_results[prompt_type] = result\n",
    "\n",
    "# Save to file\n",
    "with open(OUTPUT_DIR / \"llm_integration_results.json\", 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved comprehensive results to {OUTPUT_DIR / 'llm_integration_results.json'}\")\n",
    "\n",
    "# Save a sample for the README\n",
    "readme_example = comprehensive_results[\"detailed\"]\n",
    "with open(OUTPUT_DIR / \"readme_example.json\", 'w') as f:\n",
    "    json.dump(readme_example, f, indent=2)\n",
    "\n",
    "print(f\"Saved README example to {OUTPUT_DIR / 'readme_example.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fallback Mechanisms for API Issues\n",
    "\n",
    "Let's implement a fallback mechanism for when the OpenAI API is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def generate_answer_with_fallback(query, retrieved_docs, prompt_type=\"standard\", max_retries=3):\n",
    "    \"\"\"Generate an answer with fallback mechanisms for API issues.\"\"\"\n",
    "    prompt_template = prompt_templates.get(prompt_type, prompt_templates[\"standard\"])\n",
    "    \n",
    "    # Try OpenAI API first\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = llm_generator.generate_answer(query, retrieved_docs, prompt_template)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    \n",
    "    # If all attempts fail, use a simple extractive fallback\n",
    "    print(\"All API attempts failed. Using extractive fallback.\")\n",
    "    \n",
    "    # Simple extractive summary from top documents\n",
    "    fallback_answer = f\"Based on the retrieved documents:\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs[:3]):\n",
    "        # Extract first 200 characters from each document\n",
    "        snippet = doc[\"content\"][:200] + \"...\"\n",
    "        fallback_answer += f\"Document {i+1}: {snippet}\\n\\n\"\n",
    "    \n",
    "    fallback_answer += \"\\nNote: This is an extractive summary due to LLM API unavailability.\"\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": fallback_answer,\n",
    "        \"sources\": [doc.get(\"id\", \"unknown\") for doc in retrieved_docs],\n",
    "        \"fallback_used\": True\n",
    "    }\n",
    "\n",
    "# Example usage (we'll simulate an API failure)\n",
    "def simulate_api_failure():\n",
    "    # Get retrieval results\n",
    "    query = \"What are the best recovery strategies for athletes?\"\n",
    "    retrieval_results = rag_pipeline.query(query, top_k=3, rerank=True)\n",
    "    \n",
    "    # Format documents\n",
    "    retrieved_docs = []\n",
    "    for doc in retrieval_results['results']:\n",
    "        retrieved_docs.append({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"content\": doc[\"content\"],\n",
    "            \"metadata\": {\n",
    "                \"source\": doc.get(\"metadata\", {}).get(\"source\", \"Unknown\"),\n",
    "                \"score\": doc[\"score\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Temporarily set an invalid API key to simulate failure\n",
    "    original_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"invalid-key-for-testing\"\n",
    "    \n",
    "    try:\n",
    "        # This should fail and use the fallback\n",
    "        result = generate_answer_with_fallback(query, retrieved_docs, max_retries=1)\n",
    "        print(f\"\\nQuery: {query}\\n\")\n",
    "        print(f\"Answer:\\n{result['answer']}\")\n",
    "    finally:\n",
    "        # Restore the original key\n",
    "        if original_key:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = original_key\n",
    "        else:\n",
    "            del os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Uncomment to test the fallback mechanism\n",
    "# simulate_api_failure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully integrated an LLM with our retrieval pipeline to create a complete RAG system. Key accomplishments include:\n",
    "\n",
    "1. **Prompt Engineering**: We created different prompt templates for various query types, demonstrating how prompt design affects answer quality and length.\n",
    "\n",
    "2. **Retrieval-Augmented Generation**: We showed how grounding LLM responses in retrieved documents improves answer accuracy and relevance.\n",
    "\n",
    "3. **Comparison Analysis**: We compared answers generated with and without retrieval context, highlighting the benefits of the RAG approach.\n",
    "\n",
    "4. **Fallback Mechanisms**: We implemented fallback strategies for handling API failures, ensuring system robustness.\n",
    "\n",
    "This completes our RAG pipeline, which now includes document processing, embedding, retrieval, reranking, and answer generation components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
