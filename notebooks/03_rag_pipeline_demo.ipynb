{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete RAG (Retrieval-Augmented Generation) pipeline that combines:\n",
    "1. Document embedding using SentenceTransformers\n",
    "2. Vector search using FAISS\n",
    "3. Re-ranking using TF-IDF + cosine similarity\n",
    "\n",
    "The pipeline is designed to improve factual accuracy and reduce hallucinations in LLM responses by providing high-quality context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import our pipeline components\n",
    "from src.embedder import Embedder\n",
    "from src.vector_store import VectorStore\n",
    "from src.reranker import Reranker\n",
    "from src.rag_pipeline import RAGPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Pipeline\n",
    "\n",
    "First, we'll create our RAG pipeline with the default components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = RAGPipeline(embedding_model=\"all-MiniLM-L6-v2\")\n",
    "print(f\"Pipeline initialized with embedding model: {pipeline.embedder.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Index Documents\n",
    "\n",
    "Now we'll load our sample documents and index them in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Path to sample documents\n",
    "docs_path = Path(\"../data/sample_docs\")\n",
    "\n",
    "# Load documents\n",
    "documents = pipeline.load_documents(docs_path)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Display document IDs and titles\n",
    "for doc in documents:\n",
    "    # Extract title from first line\n",
    "    title = doc['content'].split('\\n')[0].strip('# ')\n",
    "    print(f\"Document ID: {doc['id']}, Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Index the documents\n",
    "pipeline.index_documents()\n",
    "print(f\"Documents indexed in vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query the Pipeline\n",
    "\n",
    "Let's test our pipeline with some sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# Define some test queries\n",
    "test_queries = [\n",
    "    \"What is deep learning?\",\n",
    "    \"What are the applications of NLP?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What are the challenges in natural language processing?\"\n",
    "]\n",
    "\n",
    "# Run each query and collect results\n",
    "results = []\n",
    "for query in test_queries:\n",
    "    print(f\"\\nProcessing query: '{query}'\")\n",
    "    result = pipeline.query(query, top_k=2, rerank=True)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print top result\n",
    "    if result['results']:\n",
    "        top_doc = result['results'][0]\n",
    "        print(f\"Top result: {top_doc['id']} (Score: {top_doc['score']:.3f})\")\n",
    "        print(f\"Timing: {result['timing']['total_ms']:.2f}ms total\")\n",
    "    else:\n",
    "        print(\"No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results\n",
    "\n",
    "Let's analyze the performance of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Compare with and without reranking\n",
    "query = \"What are the applications of natural language processing?\"\n",
    "\n",
    "# Without reranking\n",
    "result_no_rerank = pipeline.query(query, top_k=2, rerank=False)\n",
    "\n",
    "# With reranking\n",
    "result_with_rerank = pipeline.query(query, top_k=2, rerank=True)\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n=== Without Reranking ===\")\n",
    "for i, doc in enumerate(result_no_rerank['results']):\n",
    "    print(f\"{i+1}. {doc['id']} (Score: {doc['score']:.3f})\")\n",
    "\n",
    "print(\"\\n=== With Reranking ===\")\n",
    "for i, doc in enumerate(result_with_rerank['results']):\n",
    "    print(f\"{i+1}. {doc['id']} (Score: {doc['score']:.3f}, TF-IDF: {doc['tfidf_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Performance\n",
    "\n",
    "Let's visualize the timing breakdown of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Extract timing data\n",
    "timing_data = pd.DataFrame([\n",
    "    {\n",
    "        'query': r['query'],\n",
    "        'embedding_ms': r['timing']['query_embedding_ms'],\n",
    "        'vector_search_ms': r['timing']['vector_search_ms'],\n",
    "        'rerank_ms': r['timing']['rerank_ms'],\n",
    "        'total_ms': r['timing']['total_ms']\n",
    "    } for r in results\n",
    "])\n",
    "\n",
    "# Plot timing breakdown\n",
    "plt.figure(figsize=(10, 6))\n",
    "timing_data.set_index('query')[[\"embedding_ms\", \"vector_search_ms\", \"rerank_ms\"]].plot(kind=\"bar\", stacked=True)\n",
    "plt.title(\"Query Processing Time Breakdown\")\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results to JSON\n",
    "\n",
    "Let's save one of our query results to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Select a query result to save\n",
    "nlp_query_result = next((r for r in results if \"NLP\" in r['query']), results[0])\n",
    "\n",
    "# Save to JSON\n",
    "output_path = Path(\"../outputs/example_output.json\")\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(nlp_query_result, f, indent=2)\n",
    "    \n",
    "print(f\"Saved query result to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Pipeline\n",
    "\n",
    "Finally, let's save our pipeline for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# Save the pipeline\n",
    "pipeline_dir = Path(\"../outputs/saved_pipeline\")\n",
    "pipeline.save(pipeline_dir)\n",
    "print(f\"Pipeline saved to {pipeline_dir}\")\n",
    "\n",
    "# Test loading the pipeline\n",
    "loaded_pipeline = RAGPipeline.load(pipeline_dir)\n",
    "print(f\"Pipeline loaded successfully with {loaded_pipeline.vector_store.index.ntotal} indexed documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete hybrid retrieval pipeline that combines:\n",
    "\n",
    "1. **Embedding**: Converting documents and queries to vector representations\n",
    "2. **Vector Search**: Finding semantically similar documents using FAISS\n",
    "3. **Reranking**: Improving results with TF-IDF based reranking\n",
    "\n",
    "This pipeline can be integrated with an LLM to create a full RAG system that reduces hallucinations and improves factual accuracy by providing relevant context for generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
