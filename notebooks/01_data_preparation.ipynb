{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for RAG Pipeline\n",
    "\n",
    "This notebook demonstrates the process of preparing documents for the RAG pipeline, including:\n",
    "1. Loading and parsing documents\n",
    "2. Text cleaning and preprocessing\n",
    "3. Document chunking strategies\n",
    "4. Metadata extraction and enrichment\n",
    "\n",
    "These steps are critical for ensuring high-quality retrieval in the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Add the src directory to the path\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "REAL_DOCS_DIR = DATA_DIR / \"real_docs\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Loading\n",
    "\n",
    "First, we'll load all the documents from our real_docs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "def load_documents(directory: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load documents from a directory.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Get all text files\n",
    "    files = list(directory.glob(\"*.txt\"))\n",
    "    \n",
    "    for file_path in tqdm(files, desc=\"Loading documents\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "            # Extract title from first line (assuming markdown format)\n",
    "            lines = content.split('\\n')\n",
    "            title = lines[0].strip('# ') if lines and lines[0].startswith('#') else file_path.stem\n",
    "            \n",
    "            documents.append({\n",
    "                'id': file_path.stem,\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'metadata': {\n",
    "                    'source': str(file_path),\n",
    "                    'filename': file_path.name,\n",
    "                    'created_at': os.path.getctime(file_path),\n",
    "                    'file_size': os.path.getsize(file_path)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents(REAL_DOCS_DIR)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Display document info\n",
    "doc_info = pd.DataFrame([\n",
    "    {'id': doc['id'], 'title': doc['title'], 'length': len(doc['content'])}\n",
    "    for doc in documents\n",
    "])\n",
    "doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning and Preprocessing\n",
    "\n",
    "Now we'll clean and preprocess the document text to improve retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Basic text cleaning.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "    \n",
    "    # Replace multiple newlines with single newline\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_documents(documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Preprocess all documents.\"\"\"\n",
    "    processed_docs = []\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Preprocessing documents\"):\n",
    "        # Create a copy of the document\n",
    "        processed_doc = doc.copy()\n",
    "        \n",
    "        # Clean the content\n",
    "        processed_doc['content'] = clean_text(doc['content'])\n",
    "        \n",
    "        # Add preprocessing metadata\n",
    "        processed_doc['metadata']['word_count'] = len(processed_doc['content'].split())\n",
    "        processed_doc['metadata']['preprocessed'] = True\n",
    "        \n",
    "        processed_docs.append(processed_doc)\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "# Preprocess documents\n",
    "processed_documents = preprocess_documents(documents)\n",
    "\n",
    "# Compare original vs preprocessed length\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'id': doc['id'],\n",
    "        'original_length': len(documents[i]['content']),\n",
    "        'processed_length': len(doc['content']),\n",
    "        'word_count': doc['metadata']['word_count']\n",
    "    }\n",
    "    for i, doc in enumerate(processed_documents)\n",
    "])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Chunking\n",
    "\n",
    "For effective retrieval, we'll chunk the documents into smaller, more focused pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def chunk_document_by_paragraphs(doc: Dict[str, Any], min_chunk_size: int = 100, max_chunk_size: int = 512) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Chunk document by paragraphs with size constraints.\"\"\"\n",
    "    content = doc['content']\n",
    "    \n",
    "    # Split by double newlines (paragraphs)\n",
    "    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', content) if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # If paragraph is too long, split it by sentences\n",
    "        if len(para.split()) > max_chunk_size:\n",
    "            sentences = nltk.sent_tokenize(para)\n",
    "            for sent in sentences:\n",
    "                if len(current_chunk.split()) + len(sent.split()) <= max_chunk_size:\n",
    "                    current_chunk += \" \" + sent if current_chunk else sent\n",
    "                else:\n",
    "                    if current_chunk:  # Save current chunk if not empty\n",
    "                        if len(current_chunk.split()) >= min_chunk_size:\n",
    "                            chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sent\n",
    "        else:\n",
    "            # Check if adding this paragraph exceeds max size\n",
    "            if len(current_chunk.split()) + len(para.split()) <= max_chunk_size:\n",
    "                current_chunk += \"\\n\\n\" + para if current_chunk else para\n",
    "            else:\n",
    "                if current_chunk:  # Save current chunk if not empty\n",
    "                    if len(current_chunk.split()) >= min_chunk_size:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                current_chunk = para\n",
    "    \n",
    "    # Add the last chunk if not empty\n",
    "    if current_chunk and len(current_chunk.split()) >= min_chunk_size:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    # Create chunk documents\n",
    "    chunk_docs = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_doc = {\n",
    "            'id': f\"{doc['id']}_chunk_{i+1}\",\n",
    "            'title': f\"{doc['title']} (Part {i+1})\",\n",
    "            'content': chunk,\n",
    "            'metadata': {\n",
    "                **doc['metadata'],\n",
    "                'parent_id': doc['id'],\n",
    "                'chunk_id': i+1,\n",
    "                'chunk_count': len(chunks),\n",
    "                'word_count': len(chunk.split())\n",
    "            }\n",
    "        }\n",
    "        chunk_docs.append(chunk_doc)\n",
    "    \n",
    "    return chunk_docs\n",
    "\n",
    "def chunk_all_documents(documents: List[Dict[str, Any]], min_size: int = 100, max_size: int = 512) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Chunk all documents.\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Chunking documents\"):\n",
    "        doc_chunks = chunk_document_by_paragraphs(doc, min_chunk_size=min_size, max_chunk_size=max_size)\n",
    "        all_chunks.extend(doc_chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Chunk documents\n",
    "chunked_documents = chunk_all_documents(processed_documents, min_size=100, max_size=300)\n",
    "print(f\"Created {len(chunked_documents)} chunks from {len(processed_documents)} documents\")\n",
    "\n",
    "# Display chunk statistics\n",
    "chunk_stats = pd.DataFrame([\n",
    "    {\n",
    "        'chunk_id': doc['id'],\n",
    "        'parent_id': doc['metadata']['parent_id'],\n",
    "        'word_count': doc['metadata']['word_count'],\n",
    "        'chunk_number': doc['metadata']['chunk_id'],\n",
    "        'total_chunks': doc['metadata']['chunk_count']\n",
    "    }\n",
    "    for doc in chunked_documents\n",
    "])\n",
    "\n",
    "# Display summary statistics\n",
    "chunk_stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metadata Enrichment\n",
    "\n",
    "Let's enrich our document metadata to improve retrieval relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def extract_topics(text: str, n_keywords: int = 5) -> List[str]:\n",
    "    \"\"\"Extract main topics/keywords from text.\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    # Get stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Create vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
    "    \n",
    "    # Fit on the single document\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top keywords\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "    top_indices = tfidf_scores.argsort()[-n_keywords:][::-1]\n",
    "    top_keywords = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "def categorize_document(text: str) -> str:\n",
    "    \"\"\"Simple categorization based on keyword presence.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    categories = {\n",
    "        'hrv': ['hrv', 'heart rate variability', 'rmssd', 'parasympathetic'],\n",
    "        'training': ['training', 'workout', 'exercise', 'intensity', 'volume'],\n",
    "        'recovery': ['recovery', 'rest', 'adaptation', 'supercompensation'],\n",
    "        'sleep': ['sleep', 'rem', 'deep sleep', 'circadian'],\n",
    "        'nutrition': ['nutrition', 'diet', 'protein', 'carbohydrate', 'hydration'],\n",
    "        'stress': ['stress', 'overtraining', 'fatigue', 'burnout']\n",
    "    }\n",
    "    \n",
    "    # Count keyword matches for each category\n",
    "    category_scores = {}\n",
    "    for category, keywords in categories.items():\n",
    "        score = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "        category_scores[category] = score\n",
    "    \n",
    "    # Get category with highest score\n",
    "    if max(category_scores.values()) > 0:\n",
    "        return max(category_scores.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "def enrich_document_metadata(doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Enrich document with additional metadata.\"\"\"\n",
    "    # Create a copy\n",
    "    enriched_doc = doc.copy()\n",
    "    \n",
    "    # Extract topics\n",
    "    topics = extract_topics(doc['content'])\n",
    "    \n",
    "    # Categorize document\n",
    "    category = categorize_document(doc['content'])\n",
    "    \n",
    "    # Add to metadata\n",
    "    enriched_doc['metadata']['topics'] = topics\n",
    "    enriched_doc['metadata']['category'] = category\n",
    "    \n",
    "    # Add reading time estimate (assuming 200 words per minute)\n",
    "    word_count = doc['metadata'].get('word_count', len(doc['content'].split()))\n",
    "    enriched_doc['metadata']['reading_time_minutes'] = round(word_count / 200, 1)\n",
    "    \n",
    "    return enriched_doc\n",
    "\n",
    "# Enrich document metadata\n",
    "enriched_documents = []\n",
    "for doc in tqdm(chunked_documents, desc=\"Enriching metadata\"):\n",
    "    enriched_doc = enrich_document_metadata(doc)\n",
    "    enriched_documents.append(enriched_doc)\n",
    "\n",
    "# Display enriched metadata\n",
    "metadata_df = pd.DataFrame([\n",
    "    {\n",
    "        'id': doc['id'],\n",
    "        'category': doc['metadata']['category'],\n",
    "        'topics': ', '.join(doc['metadata']['topics']),\n",
    "        'reading_time': doc['metadata']['reading_time_minutes']\n",
    "    }\n",
    "    for doc in enriched_documents\n",
    "])\n",
    "\n",
    "# Show category distribution\n",
    "category_counts = metadata_df['category'].value_counts()\n",
    "category_counts.plot(kind='bar', figsize=(10, 5))\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Documents\n",
    "\n",
    "Finally, let's save our processed documents for use in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Create processed directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Save processed documents\n",
    "with open(PROCESSED_DIR / \"processed_chunks.json\", 'w') as f:\n",
    "    json.dump(enriched_documents, f, indent=2)\n",
    "\n",
    "# Also save as JSONL for easier loading\n",
    "with open(PROCESSED_DIR / \"processed_chunks.jsonl\", 'w') as f:\n",
    "    for doc in enriched_documents:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "print(f\"Saved {len(enriched_documents)} processed document chunks to {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Statistics and Analysis\n",
    "\n",
    "Let's analyze our processed documents to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Word count distribution\n",
    "word_counts = [doc['metadata']['word_count'] for doc in enriched_documents]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(word_counts, bins=20)\n",
    "plt.title('Document Chunk Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(word_counts), color='red', linestyle='--', label=f'Mean: {np.mean(word_counts):.1f}')\n",
    "plt.axvline(np.median(word_counts), color='green', linestyle='--', label=f'Median: {np.median(word_counts):.1f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Category distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y=metadata_df['category'], order=metadata_df['category'].value_counts().index)\n",
    "plt.title('Document Category Distribution')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Topic analysis\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten all topics\n",
    "all_topics = [topic for doc in enriched_documents for topic in doc['metadata']['topics']]\n",
    "topic_counts = Counter(all_topics)\n",
    "\n",
    "# Plot top 20 topics\n",
    "top_topics = pd.DataFrame(topic_counts.most_common(20), columns=['Topic', 'Count'])\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Count', y='Topic', data=top_topics)\n",
    "plt.title('Top 20 Topics Across All Documents')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully prepared our documents for the RAG pipeline by:\n",
    "1. Loading and parsing the raw documents\n",
    "2. Cleaning and preprocessing the text\n",
    "3. Chunking documents into appropriate sizes\n",
    "4. Enriching metadata with topics, categories, and other useful information\n",
    "\n",
    "These processed documents are now ready for embedding and indexing in our vector store."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
