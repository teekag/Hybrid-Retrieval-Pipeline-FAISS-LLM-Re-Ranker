{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Retrieval Evaluation ‚Äì RAG Pipeline\n",
    "\n",
    "This notebook evaluates the quality of document retrieval using metrics like:\n",
    "\n",
    "- **Precision@k**\n",
    "- **Recall@k**\n",
    "- **Mean Reciprocal Rank (MRR)**\n",
    "- **nDCG (normalized Discounted Cumulative Gain)**\n",
    "\n",
    "We compare the performance of:\n",
    "1. Vector search only (baseline)\n",
    "2. Vector search + TF-IDF reranker (hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Simulated relevance judgments (1 = relevant, 0 = not relevant)\n",
    "ground_truth = {\n",
    "    \"What are the effects of low HRV?\": [\"doc_hrv.txt\", \"doc_recovery.txt\"]\n",
    "}\n",
    "\n",
    "def precision_at_k(predicted_docs, relevant_docs, k):\n",
    "    predicted_k = predicted_docs[:k]\n",
    "    return len(set(predicted_k) & set(relevant_docs)) / k\n",
    "\n",
    "def recall_at_k(predicted_docs, relevant_docs, k):\n",
    "    predicted_k = predicted_docs[:k]\n",
    "    return len(set(predicted_k) & set(relevant_docs)) / len(relevant_docs)\n",
    "\n",
    "def mrr(predicted_docs, relevant_docs):\n",
    "    for rank, doc in enumerate(predicted_docs, start=1):\n",
    "        if doc in relevant_docs:\n",
    "            return 1 / rank\n",
    "    return 0\n",
    "\n",
    "def dcg(scores):\n",
    "    return sum((score / np.log2(idx + 2)) for idx, score in enumerate(scores))\n",
    "\n",
    "def ndcg(predicted_docs, relevant_docs):\n",
    "    relevance = [1 if doc in relevant_docs else 0 for doc in predicted_docs]\n",
    "    ideal = sorted(relevance, reverse=True)\n",
    "    return dcg(relevance) / dcg(ideal) if dcg(ideal) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated outputs for a query\n",
    "baseline_results = [\"doc_sleep.txt\", \"doc_hrv.txt\", \"doc_noise.txt\"]\n",
    "reranked_results = [\"doc_hrv.txt\", \"doc_recovery.txt\", \"doc_sleep.txt\"]\n",
    "\n",
    "query = \"What are the effects of low HRV?\"\n",
    "relevant = ground_truth[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Baseline\n",
      "Precision@3: 0.33\n",
      "Recall@3:    0.50\n",
      "MRR:         0.50\n",
      "nDCG:        0.63\n",
      "------------------------------\n",
      "üìå Reranked\n",
      "Precision@3: 0.67\n",
      "Recall@3:    1.00\n",
      "MRR:         1.00\n",
      "nDCG:        1.00\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, results in [(\"Baseline\", baseline_results), (\"Reranked\", reranked_results)]:\n",
    "    print(f\"üìå {name}\")\n",
    "    print(f\"Precision@3: {precision_at_k(results, relevant, 3):.2f}\")\n",
    "    print(f\"Recall@3:    {recall_at_k(results, relevant, 3):.2f}\")\n",
    "    print(f\"MRR:         {mrr(results, relevant):.2f}\")\n",
    "    print(f\"nDCG:        {ndcg(results, relevant):.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Summary\n",
    "\n",
    "The reranked results show higher retrieval quality across all key metrics, demonstrating the value of combining dense retrieval (FAISS) with shallow re-ranking (TF-IDF cosine similarity).\n",
    "\n",
    "Next steps:\n",
    "- Try different reranking strategies (e.g., BERTScore, LLM-assisted)\n",
    "- Test across multiple queries and document types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
